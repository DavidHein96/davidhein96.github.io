[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "Welcome to my site! I just graduated with my MS in data science and have a background in academic medical research. My work involves a full stack of end-to-end skills, from utilizing efficient data structures, assessing bioinformatics tools, navigating clinical healthcare workflows, writing academic manuscripts, to the final communication of findings to stakeholders. Outside of my current position I’m interested in learning to maximize the utility of large language models, and love teaching people what I’ve found!\n\nTechnical Skills & Projects\nCheck out my projects, which are tagged with technical skill categories. As a bonus, here’s my introductory guide to ChatGPT. Many of my coding projects for my degree and my research position are in private repositories, feel free to reach out for more information regarding code samples.\n\n\nEducation\nUniversity of Texas at Austin\nMaster of Science in Data Science | 2021-2023\n\nCompleted coursework in deep learning, reinforcement learning, and natural language processing\n\nBachelor of Science in Chemistry | 2015-2019\n\nMcCombs Business Foundations Certificate\nGraduated with High Honors\n\n\n\nExperience\nUniversity of Texas Southwestern Medical Center\nClinical Data Specialist/Research Assistant | 2020 - Present\n\nPublished, as first author, multiple papers concerning colorectal cancer epidemiology and biology with an emphasis on racial and ethnic equity\nAchieved significant reductions in compute time and cost for several data processing pipelines utilizing R, Python, Bash, and high-performance computing environments\nLead research initiatives involving the collection, processing, analysis, and reporting of data from diverse sources such as public databases, clinical trials, electronic medical records, and genetic sequencing results\nEngaged in cross-sector collaboration with academic labs, such as UTSW’s Medical Artificial Intelligence and Automation Lab, and private sector partners, such as Tempus Labs, in projects that benefited from pooled resources\nContributed as subject matter expert on additional projects and publications by providing guidance to department faculty in statistical methods and complex data analysis\n\n\n\nPublications & Presentations\nHein D, Rhead B, Pouliot Y, Guinney J, De La Vega F, Sanford N. Racial and Genetic Ancestry Associations with Gene Expression Patterns in a Real-World Cohort of Colorectal Cancer Patients. American Society of Clinical Oncology Annual Conference Poster Presentation. 2023 Publication Link | My Projects Link\nHein D, Coughlin L, Poulides N, Koh A, Sanford N. Microbiome and Genomic Profiling in Rectal Adenocarcinoma. American Association for Cancer Research Annual Conference Poster Poster Presentation. 2023 Publication Link | My Projects Link\nHein D, Deng W, Bleile M, Kazmi S, Jones A, Kainthla R, Jiang W, Cantarel B, Sanford N. Racial and Ethnic Differences in Genomic Profiling of Early Onset Colorectal Cancer. Journal of the National Cancer Institute. 2022 Publication Link\nRhead B, Hein D, Pouliot Y, Guinney J, De La Vega F, Sanford N. Genetic Ancestry Differences in Tumor Mutation in Early and Average-Onset Colorectal Cancer. ASCO Annual Conference Poster Presentation. 2022 Publication Link\nHein D, Jones A, Chul A, Sanford N. Self-reported Reasons for Colonoscopy Among Adults Aged 45-49 versus 50 Years and Older from 2010-2018. Cancer Epidemiology. 2021 Publication Link\nHein D, Ahn C, Aguilera T, Folkert M, Sanford N. Trends and Factors Associated with Receipt of Upfront Surgery for Stage II-III Rectal Adenocarcinoma in the United States, 2006-2016. American Journal of Clinical Oncology. 2021 Publication Link | My Projects Link\nZhang-Velten E, Eraj S, Hein D, Aguilera T, Folkert M, Sanford N. Patterns of Dose Escalation Among Patients with Esophageal Cancer Undergoing Definitive Radiation Therapy: 2006-2016. Advances in Radiation Oncology. 2020 Publication Link"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/first_project/index.html",
    "href": "posts/first_project/index.html",
    "title": "My First Paper: Trends and Associations in Rectal Cancer Treatment",
    "section": "",
    "text": "This project demonstrates code used in the data cleaning, analysis, and generation of figures in my verfy first publication “Trends and Factors Associated with Receipt of Upfront Surgery for Stage II-III Rectal Adenocarcinoma in the United States”. Here’s a link to the abstract. This blog was created with Quarto in R Studio, so to include Python versions of analysis and graphics I needed to specify an Anaconda environment in the setup chunk.\n\n\nCode\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(lmtest)\nlibrary(glmnet)\nlibrary(nnet)\nlibrary(reticulate)\nlibrary(broom)\nlibrary(rmarkdown)\nlibrary(knitr)\n\nuse_condaenv(\"website_env\", required = TRUE)\nknitr::opts_chunk$set(python.reticulate = FALSE)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.dfghsfdgsgfds\n\nprint(\"hi\")\n\n[1] \"hi\""
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Basic Convolutional Neural Network For Classification\n\n\n\n\n\n\n\nPython\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nDave Hein\n\n\n\n\n\n\n  \n\n\n\n\nMy First Paper: Trends and Associations in Rectal Cancer Treatment\n\n\n\n\n\n\n\nR\n\n\nPython\n\n\nStatistics\n\n\nSynthetic data\n\n\nData cleaning\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nDave Hein\n\n\n\n\n\n\n  \n\n\n\n\nASCO Poster: Genetic Ancestry and Gene Expression in CRC\n\n\n\n\n\n\n\nR\n\n\nBioinformatics\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nDave Hein\n\n\n\n\n\n\n  \n\n\n\n\nAACR Poster: Microbiome and Tumor Gene Expression\n\n\n\n\n\n\n\nR\n\n\nBioinformatics\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2023\n\n\nDave Hein\n\n\n\n\n\n\n  \n\n\n\n\nGPT-4 Everyone\n\n\n\n\n\n\n\nNatural language processing\n\n\nTechnical communication\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2023\n\n\nDave Hein\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/first_project/index.html#creating-synthetic-data",
    "href": "posts/first_project/index.html#creating-synthetic-data",
    "title": "My First Paper: Trends and Associations in Rectal Cancer Treatment",
    "section": "Creating Synthetic Data",
    "text": "Creating Synthetic Data\nThe original paper uses data from the National Cancer Data Base. This data is not public thus I created a synthetic data set that attempts to recreate the characteristics of the original data but with much greater anonymization.\n\nClean and Prep Original Data\nFirst step was to clean and engineer variables as in the original paper. The source file is retained on a secure server and these code chunks were only ran once (eval=FALSE)\n\n\nCode\n# We will use synthpop to generate a synthetic data set\nlibrary(synthpop)\n\n# Reading in original data from the NCDB\nRectalData &lt;- read_dta(\"ncdb_project_data.dta\")\n\ndf1 &lt;- RectalData %&gt;%\n    select(RX_SUMM_SYSTEMIC_SUR_SEQ, RX_SUMM_RADIATION, RX_SUMM_SURGRAD_SEQ, RX_SUMM_CHEMO,\n        SEX, MED_INC_QUAR_12, AGE, raceb, TNM_CLIN_STAGE_GROUP, TNM_PATH_STAGE_GROUP,\n        SEQUENCE_NUMBER, HISTOLOGY, RX_SUMM_SURG_PRIM_SITE, YEAR_OF_DIAGNOSIS, FACILITY_TYPE_CD,\n        INSURANCE_STATUS, CDCC_TOTAL_BEST, DX_DEFSURG_STARTED_DAYS, CLASS_OF_CASE)\n\n# Remove unknown sequences, treatments, and keep proper hist codes & primary\n# surgery\ndf1 &lt;- df1 %&gt;%\n    filter(!RX_SUMM_SYSTEMIC_SUR_SEQ %in% c(6, 7, 9), !RX_SUMM_RADIATION == 9, !RX_SUMM_SURGRAD_SEQ %in%\n        c(6, 9), !RX_SUMM_CHEMO %in% c(99, 88), SEQUENCE_NUMBER %in% c(\"00\", \"01\"),\n        HISTOLOGY %in% c(8140, 8210, 8260, 8261, 8262, 8263, 8470, 8480, 8481), RX_SUMM_SURG_PRIM_SITE %in%\n            c(30, 40, 50, 60, 70, 80))\n\n# Make a simple stage variable and filter to only stage 2 and 3\ndf1 &lt;- df1 %&gt;%\n    mutate(TNM_CLIN_STAGE_GROUP, stage3 = ifelse(str_detect(TNM_CLIN_STAGE_GROUP,\n        \"2\"), \"2\", \"0\"))\ndf1$stage3[str_detect(df1$TNM_CLIN_STAGE_GROUP, \"3\") == TRUE] &lt;- \"3\"\ndf1 &lt;- df1[!(df1$stage3 == \"0\"), ]\n\n# Define neoadjuvant, rad or chemo before surgery\ndf1 &lt;- df1 %&gt;%\n    mutate(neoadjuvant = 3)\ndf1 &lt;- df1 %&gt;%\n    mutate(neoadjuvant = case_when(RX_SUMM_SURGRAD_SEQ %in% c(2, 4) ~ 1, RX_SUMM_SYSTEMIC_SUR_SEQ %in%\n        c(2, 4) ~ 1, RX_SUMM_RADIATION == 0 & RX_SUMM_CHEMO %in% c(0, 82, 85, 86,\n        87) ~ 0, RX_SUMM_SURGRAD_SEQ == 3 & RX_SUMM_CHEMO %in% c(0, 82, 85, 86, 87) ~\n        0, RX_SUMM_SYSTEMIC_SUR_SEQ == 3 & df1$RX_SUMM_RADIATION == 0 ~ 0, RX_SUMM_SURGRAD_SEQ ==\n        3 & RX_SUMM_SYSTEMIC_SUR_SEQ == 3 ~ 0)) %&gt;%\n    filter(neoadjuvant != 3)\n\n# age group under 45, 45-65, 65 and up AGE\ndf1 &lt;- df1 %&gt;%\n    mutate(agegroup = case_when(AGE &lt;= 45 ~ 0, AGE &gt; 45 & AGE &lt; 65 ~ 1, AGE &gt;= 65 ~\n        2))\n\n# create a var for switching facility and add the unknown gov insurance to\n# unknown insurance\ndf1 &lt;- df1 %&gt;%\n    mutate(CLASS_OF_CASE, treatnotatdiag = ifelse(CLASS_OF_CASE %in% c(0, 20, 21,\n        22), \"Switched\", \"Stayed\"), INSURANCE_STATUS = ifelse(INSURANCE_STATUS ==\n        4, 9, INSURANCE_STATUS))\n\n# Omit NA, this project used complete case analysis\ndf1 &lt;- na.omit(df1)\n\n# Keep only relevant variables\ndf1 &lt;- df1 %&gt;%\n    select(SEX, MED_INC_QUAR_12, agegroup, raceb, stage3, neoadjuvant, treatnotatdiag,\n        YEAR_OF_DIAGNOSIS, FACILITY_TYPE_CD, INSURANCE_STATUS, CDCC_TOTAL_BEST, DX_DEFSURG_STARTED_DAYS)\n\n# Make variables into factors\ndf2 &lt;- mutate(across(!DX_DEFSURG_STARTED_DAYS, as.factor))\n\n\n\n\nGenerate Synthetic Data\nI now use the package synthpop to create a synthetic data set using bootstrapping and classification and regression trees. I utilized stratification and changed the order of variables used in the synthesis process to create a data set with lower mean square error. Finally I remove all rows that happen to be identical to rows in the original data to ensure every row is synthetic.\n\n\nCode\n# Make synthetic data using CART and bootstrapping\n\n# Reorder for synthetic data generation\ndf3 &lt;- df2 %&gt;%\n    dplyr::relocate(raceb, MED_INC_QUAR_12, neoadjuvant)\n\n# Strat by race and ethnicity and median income for best overlap\nsynth_1 &lt;- syn.strata(df3, proper = TRUE, strata = df2$raceb)\nsynth_2 &lt;- syn.strata(df3, proper = TRUE, strata = df3$MED_INC_QUAR_12)\n\nsynth_dat1 &lt;- synth_1$syn\nsynth_dat2 &lt;- synth_2$syn\n\nvars_i_want &lt;- colnames(df2)\nvars_i_want &lt;- vars_i_want[!vars_i_want == \"DX_DEFSURG_STARTED_DAYS\"]\n\n# Check Heat maps to compare pMSE\nsynthpop::utility.tables(synth_dat1, df3, tables = \"twoway\", vars = vars_i_want)\nsynthpop::utility.tables(synth_dat2, df3, tables = \"twoway\", vars = vars_i_want)\n\n# Remove Replicates & tag for synthetic\nfinal_synth_data &lt;- sdc(synth_2, df3, label = \"synthetic\", rm.replicated.uniques = TRUE)\nf &lt;- final_synth_data$syn\nwrite_tsv(f, \"synthetic_NCDB_data.tsv\")"
  },
  {
    "objectID": "posts/first_project/index.html#analysis",
    "href": "posts/first_project/index.html#analysis",
    "title": "Adoption of a new Colorectal Cancer Treatment",
    "section": "Analysis",
    "text": "Analysis\n\nLogistic Regression\n\nIn PythonIn R\n\n\n\n\nCode\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\n\nsynthetic_NCDB_data = pd.read_csv(\"synthetic_NCDB_data.tsv\", sep = '\\t')\n\n# Logistic Regression for odds of upfront surgery\n\nmodel = smf.logit(formula=\"neoadjuvant~ C(YEAR_OF_DIAGNOSIS) + C(stage3) + C(MED_INC_QUAR_12) + C(INSURANCE_STATUS) + C(raceb) + C(CDCC_TOTAL_BEST) + C(SEX) + C(agegroup) + C(FACILITY_TYPE_CD) + C(treatnotatdiag)\",data = synthetic_NCDB_data).fit()\n\n#print(model.summary())\n\n# Graphing proportion receiving upfront surg at different facility types over time\n#rcParams['figure.figsize'] = 10, 8\n\n\ndfCC = synthetic_NCDB_data.query('FACILITY_TYPE_CD==1').filter(['YEAR_OF_DIAGNOSIS','neoadjuvant']).groupby(['YEAR_OF_DIAGNOSIS']).agg(['mean'])\ndfCCC = synthetic_NCDB_data.query('FACILITY_TYPE_CD==2').filter(['YEAR_OF_DIAGNOSIS','neoadjuvant']).groupby(['YEAR_OF_DIAGNOSIS']).agg(['mean'])\ndfA = synthetic_NCDB_data.query('FACILITY_TYPE_CD==3').filter(['YEAR_OF_DIAGNOSIS','neoadjuvant']).groupby(['YEAR_OF_DIAGNOSIS']).agg(['mean'])\ndfIN = synthetic_NCDB_data.query('FACILITY_TYPE_CD==4').filter(['YEAR_OF_DIAGNOSIS','neoadjuvant']).groupby(['YEAR_OF_DIAGNOSIS']).agg(['mean'])\n\n#plt.plot(dfCC, label=\"Community Cancer Center\")\n#plt.plot(dfCCC, label=\"Comprehensive Community Cancer Center\")\n#plt.plot(dfA, label=\"Academic Program\")\n#plt.plot(dfIN, label=\"Integrated Network Cancer Program\")\n#plt.legend()\n#plt.xlabel(\"Year of Diagnosis\")\n#plt.ylabel(\"Proportion Receiving Upfront Surgery\")\n#plt.show()\n\n\nOptimization terminated successfully.\n         Current function value: 0.379420\n         Iterations 6\n\n\n\n\n\n\nCode\n# Load synthetic data\nsynthetic_NCDB_data &lt;- read_delim(\"synthetic_NCDB_data.tsv\", delim = \"\\t\", escape_double = FALSE,\n    trim_ws = TRUE)\nsynthetic_NCDB_data &lt;- synthetic_NCDB_data %&gt;%\n    mutate(across(!DX_DEFSURG_STARTED_DAYS, as.factor))\n\n# logistic regression\nlog_fit &lt;- glm(neoadjuvant ~ YEAR_OF_DIAGNOSIS + MED_INC_QUAR_12 + INSURANCE_STATUS +\n    raceb + CDCC_TOTAL_BEST + SEX + agegroup + stage3 + FACILITY_TYPE_CD + treatnotatdiag,\n    data = synthetic_NCDB_data, family = binomial(link = logit))\n\n# Tidy results\ntidy_log_fit &lt;- data.frame(summary(log_fit)$coefficients)\n\nrmarkdown::paged_table(tidy_log_fit %&gt;%\n    mutate(OR = exp(Estimate)) %&gt;%\n    arrange(Pr...z..))\n\n\n\n\n  \n\n\n\nCode\n# ?paged_table\n\n\n\n\n\n\n\nGraphing proportion receiving upfront surg at different facility types over time\n\n\nCode\n# Making df of just people who got upfront surg\nsynthetic_NCDB_data &lt;- synthetic_NCDB_data %&gt;%\n    mutate(neoadjuvant = as.numeric(neoadjuvant) - 1)\ndf2 &lt;- synthetic_NCDB_data %&gt;%\n    filter(neoadjuvant == 1)\n\n# renaming the fac type\ndfyear &lt;- synthetic_NCDB_data %&gt;%\n    dplyr::select(YEAR_OF_DIAGNOSIS, neoadjuvant, FACILITY_TYPE_CD)\ndfyear$Facility_Type[dfyear$FACILITY_TYPE_CD == 1] &lt;- \"Community Cancer Program\"\n\n\nWarning: Unknown or uninitialised column: `Facility_Type`.\n\n\nCode\ndfyear$Facility_Type[dfyear$FACILITY_TYPE_CD == 2] &lt;- \"Comprehensive CC Program\"\ndfyear$Facility_Type[dfyear$FACILITY_TYPE_CD == 3] &lt;- \"Academic Program\"\ndfyear$Facility_Type[dfyear$FACILITY_TYPE_CD == 4] &lt;- \"Integrated Network Cancer Program\"\n\n\n# finding proportion of upfront surg by fac type\ndfyear &lt;- dfyear %&gt;%\n    dplyr::group_by(YEAR_OF_DIAGNOSIS, Facility_Type) %&gt;%\n    dplyr::summarize(ratio = mean(neoadjuvant))\n\n\n`summarise()` has grouped output by 'YEAR_OF_DIAGNOSIS'. You can override using\nthe `.groups` argument.\n\n\nCode\n# finding overall median per year of upfront surg\ndfyearavg &lt;- synthetic_NCDB_data %&gt;%\n    dplyr::group_by(YEAR_OF_DIAGNOSIS) %&gt;%\n    dplyr::summarize(ratio = mean(neoadjuvant))\ndfyearavg$Facility_Type &lt;- \"Overall Median\"\ndfyearavg &lt;- dfyearavg %&gt;%\n    select(YEAR_OF_DIAGNOSIS, Facility_Type, ratio)\n\n# Merging overall and by facility type\nyear1 &lt;- as.data.frame(dfyear)\nyear2 &lt;- as.data.frame(dfyearavg)\nmergedyearly &lt;- rbind(year1, year2)\nmergedyearly$Facility_Type &lt;- factor(mergedyearly$Facility_Type, levels = c(\"Integrated Network Cancer Program\",\n    \"Academic Program\", \"Overall Median\", \"Comprehensive CC Program\", \"Community Cancer Program\"))\n\n# plotting upfron surg by fac type\nggplot(mergedyearly) + geom_line(data = mergedyearly, aes(x = YEAR_OF_DIAGNOSIS,\n    y = ratio, group = Facility_Type, color = Facility_Type), size = 1.3) + labs(x = \"Year of Diagnosis\",\n    y = \"Proportion Receiving Upfront Surgery\", color = \"Facility Type\") + theme_test() +\n    scale_color_brewer(palette = \"Dark2\")\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nLooking at how delays in surgery and facility switching are linked\n\n\nCode\n# making three groups for a surgery delay\ndf2$surggroup[df2$DX_DEFSURG_STARTED_DAYS == 0] &lt;- \"Day zero\"\ndf2$surggroup[df2$DX_DEFSURG_STARTED_DAYS &gt; 0 & df2$DX_DEFSURG_STARTED_DAYS &lt; 28] &lt;- \"Short delay\"\ndf2$surggroup[df2$DX_DEFSURG_STARTED_DAYS &gt;= 28] &lt;- \"Long Delay\"\ndf2$surggroup &lt;- factor(df2$surggroup)\n\n# plotting surg delay and switching facility\nggplot(df2, aes(x = DX_DEFSURG_STARTED_DAYS, fill = treatnotatdiag)) + geom_histogram(binwidth = 5) +\n    xlim(-10, 200) + labs(x = \"Days Untill Surgery From Diagnosis\", y = \"Count\",\n    fill = \"Facility Switch\") + theme_test() + scale_fill_brewer(palette = \"Set2\") +\n    geom_vline(xintercept = 28)"
  },
  {
    "objectID": "posts/chatGPT_book/index.html",
    "href": "posts/chatGPT_book/index.html",
    "title": "GPT-4 Everyone",
    "section": "",
    "text": "Demystify AI with this fun and easygoing introduction to ChatGPT! First, starting with a gentle introduction to language models and ChatGPT, then diving deep into prompt engineering for any situation, this guide has you covered.\nHere’s a quick look at some of the awesome things we’ll do:\n\nGenerate practice quizzes, from chemistry to the SAT, with detailed feedback\nAutomate resumes and cover letters, custom tailored for individual job applications\nQuickly write emails and draft outlines for projects at work, cutting down on tedious tasks\nTrouble shoot in excel, python, and more, with immediate and adaptive feedback\nPlan tasty meals instantly using only what’s in your kitchen\nCollaborate with AI to increase your creativity, not replace it\nLearn how to see through the hype of “Make money quick with ChatGPT” schemes commonly peddled on the internet\n\nBy the end of this handbook you’ll be well prepared for success in the new world of generative AI! Amazon link if you are interested in an ebook version"
  },
  {
    "objectID": "posts/first_project/index.html#project-setup",
    "href": "posts/first_project/index.html#project-setup",
    "title": "My First Paper: Trends and Associations in Rectal Cancer Treatment",
    "section": "",
    "text": "This project demonstrates code used in the data cleaning, analysis, and generation of figures in my verfy first publication “Trends and Factors Associated with Receipt of Upfront Surgery for Stage II-III Rectal Adenocarcinoma in the United States”. Here’s a link to the abstract. This blog was created with Quarto in R Studio, so to include Python versions of analysis and graphics I needed to specify an Anaconda environment in the setup chunk.\n\n\nCode\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(lmtest)\nlibrary(glmnet)\nlibrary(nnet)\nlibrary(reticulate)\nlibrary(broom)\nlibrary(rmarkdown)\nlibrary(knitr)\n\nuse_condaenv(\"website_env\", required = TRUE)\nknitr::opts_chunk$set(python.reticulate = FALSE)"
  },
  {
    "objectID": "posts/first_project/index.html#analysis-using-synthetic-data",
    "href": "posts/first_project/index.html#analysis-using-synthetic-data",
    "title": "My First Paper: Trends and Associations in Rectal Cancer Treatment",
    "section": "Analysis Using Synthetic Data",
    "text": "Analysis Using Synthetic Data\n\nLogistic Regression\nHere I perform logistic regression using the same variables as the original paper. For fun I put in code for both R and Python. The odds being calculated here are for receipt of neoadjuvant therapy, so the ORs are reversed in comparison to the original paper.\n\nUsing RUsing Python\n\n\n\n\nCode\n# Load synthetic data\nsynthetic_NCDB_data &lt;- read_delim(\"synthetic_NCDB_data.tsv\", delim = \"\\t\", escape_double = FALSE,\n    trim_ws = TRUE)\nsynthetic_NCDB_data &lt;- synthetic_NCDB_data %&gt;%\n    mutate(across(!DX_DEFSURG_STARTED_DAYS, as.factor))\n\n# logistic regression\nlog_fit &lt;- glm(neoadjuvant ~ YEAR_OF_DIAGNOSIS + MED_INC_QUAR_12 + INSURANCE_STATUS +\n    raceb + CDCC_TOTAL_BEST + SEX + agegroup + stage3 + FACILITY_TYPE_CD + treatnotatdiag,\n    data = synthetic_NCDB_data, family = binomial(link = logit))\n\n# Tidy results\ntidy_log_fit &lt;- data.frame(summary(log_fit)$coefficients)\nrmarkdown::paged_table(round(tidy_log_fit %&gt;%\n    mutate(OR = exp(Estimate)) %&gt;%\n    arrange(Pr...z..), 3))\n\n\n\n\n  \n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\n\nsynthetic_NCDB_data_py = pd.read_csv(\"synthetic_NCDB_data.tsv\", sep = '\\t')\n\n# Logistic Regression for odds of upfront surgery\nmodel = smf.logit(formula=\"neoadjuvant~ C(YEAR_OF_DIAGNOSIS) + C(stage3) + C(MED_INC_QUAR_12) + C(INSURANCE_STATUS) + C(raceb) + C(CDCC_TOTAL_BEST) + C(SEX) + C(agegroup) + C(FACILITY_TYPE_CD) + C(treatnotatdiag)\",data = synthetic_NCDB_data_py).fit()\n\nmod_summary = model.summary()\nmod_summary_table = pd.read_html(mod_summary.tables[1].as_html(), header=0, index_col=0)[0]\nprint(mod_summary_table)\n\n\nOptimization terminated successfully.\n         Current function value: 0.379420\n         Iterations 6\n                                 coef  std err       z  P&gt;|z|  [0.025  0.975]\nIntercept                      1.4275    0.116  12.353  0.000   1.201   1.654\nC(YEAR_OF_DIAGNOSIS)[T.2007]  -0.1400    0.070  -1.992  0.046  -0.278  -0.002\nC(YEAR_OF_DIAGNOSIS)[T.2008]  -0.0996    0.068  -1.459  0.145  -0.233   0.034\nC(YEAR_OF_DIAGNOSIS)[T.2009]  -0.0424    0.068  -0.627  0.531  -0.175   0.090\nC(YEAR_OF_DIAGNOSIS)[T.2010]   0.1894    0.069   2.747  0.006   0.054   0.325\nC(YEAR_OF_DIAGNOSIS)[T.2011]   0.1960    0.068   2.871  0.004   0.062   0.330\nC(YEAR_OF_DIAGNOSIS)[T.2012]   0.1808    0.068   2.660  0.008   0.048   0.314\nC(YEAR_OF_DIAGNOSIS)[T.2013]   0.5288    0.071   7.501  0.000   0.391   0.667\nC(YEAR_OF_DIAGNOSIS)[T.2014]   0.5490    0.070   7.843  0.000   0.412   0.686\nC(YEAR_OF_DIAGNOSIS)[T.2015]   0.6183    0.072   8.637  0.000   0.478   0.759\nC(YEAR_OF_DIAGNOSIS)[T.2016]   0.6817    0.073   9.325  0.000   0.538   0.825\nC(stage3)[T.3]                 0.3496    0.028  12.359  0.000   0.294   0.405\nC(MED_INC_QUAR_12)[T.2]        0.0421    0.044   0.954  0.340  -0.044   0.129\nC(MED_INC_QUAR_12)[T.3]        0.0099    0.044   0.228  0.820  -0.075   0.095\nC(MED_INC_QUAR_12)[T.4]        0.0199    0.043   0.460  0.646  -0.065   0.105\nC(INSURANCE_STATUS)[T.1]      -0.0009    0.072  -0.013  0.990  -0.142   0.140\nC(INSURANCE_STATUS)[T.2]      -0.0702    0.087  -0.804  0.422  -0.242   0.101\nC(INSURANCE_STATUS)[T.3]      -0.1189    0.079  -1.498  0.134  -0.275   0.037\nC(INSURANCE_STATUS)[T.9]      -0.0509    0.111  -0.460  0.645  -0.268   0.166\nC(raceb)[T.2]                  0.0297    0.052   0.568  0.570  -0.073   0.132\nC(raceb)[T.3]                 -0.1414    0.071  -1.993  0.046  -0.280  -0.002\nC(raceb)[T.4]                 -0.0020    0.132  -0.015  0.988  -0.260   0.256\nC(CDCC_TOTAL_BEST)[T.1]       -0.1870    0.035  -5.320  0.000  -0.256  -0.118\nC(CDCC_TOTAL_BEST)[T.2]       -0.1247    0.067  -1.871  0.061  -0.255   0.006\nC(CDCC_TOTAL_BEST)[T.3]       -0.2123    0.098  -2.155  0.031  -0.405  -0.019\nC(SEX)[T.2]                   -0.2803    0.028  -9.893  0.000  -0.336  -0.225\nC(agegroup)[T.1]              -0.2044    0.064  -3.219  0.001  -0.329  -0.080\nC(agegroup)[T.2]              -0.5308    0.072  -7.399  0.000  -0.671  -0.390\nC(FACILITY_TYPE_CD)[T.2]       0.2747    0.048   5.763  0.000   0.181   0.368\nC(FACILITY_TYPE_CD)[T.3]       0.3958    0.050   7.912  0.000   0.298   0.494\nC(FACILITY_TYPE_CD)[T.4]       0.5036    0.058   8.725  0.000   0.390   0.617\nC(treatnotatdiag)[T.Switched]  0.4072    0.029  14.121  0.000   0.351   0.464\n\n\n\n\n\n\n\nProportions Over Time\nThis figure shows the proportion of patients receiving neoadjuvant treatment by facility type, and how this proportion has increased over time. There were a lot of issues with getting the Python figure to display correctly, so I had to do a little work around and save the figure before knitting the markdown document. Apologies for the colors not matching in the R and Python versions. There’s only so much time in the world…\n\nUsing RUsing Python\n\n\n\n\nCode\n# Making df of just people who got upfront surg\nsynthetic_NCDB_data2 &lt;- synthetic_NCDB_data %&gt;%\n    mutate(neoadjuvant = as.numeric(neoadjuvant) - 1)\ndf2 &lt;- synthetic_NCDB_data2 %&gt;%\n    filter(neoadjuvant == 0)\n\n# renaming the fac type\ndfyear &lt;- synthetic_NCDB_data2 %&gt;%\n    dplyr::select(YEAR_OF_DIAGNOSIS, neoadjuvant, FACILITY_TYPE_CD)\ndfyear$Facility_Type[dfyear$FACILITY_TYPE_CD == 1] &lt;- \"Community Cancer Program\"\ndfyear$Facility_Type[dfyear$FACILITY_TYPE_CD == 2] &lt;- \"Comprehensive CC Program\"\ndfyear$Facility_Type[dfyear$FACILITY_TYPE_CD == 3] &lt;- \"Academic Program\"\ndfyear$Facility_Type[dfyear$FACILITY_TYPE_CD == 4] &lt;- \"Integrated Network Cancer Program\"\n\n# finding proportion of upfront surg by fac type\ndfyear &lt;- dfyear %&gt;%\n    dplyr::group_by(YEAR_OF_DIAGNOSIS, Facility_Type) %&gt;%\n    dplyr::summarize(ratio = mean(neoadjuvant))\n\n# finding overall median per year of upfront surg\ndfyearavg &lt;- synthetic_NCDB_data2 %&gt;%\n    dplyr::group_by(YEAR_OF_DIAGNOSIS) %&gt;%\n    dplyr::summarize(ratio = mean(neoadjuvant))\ndfyearavg$Facility_Type &lt;- \"Overall Median\"\ndfyearavg &lt;- dfyearavg %&gt;%\n    select(YEAR_OF_DIAGNOSIS, Facility_Type, ratio)\n\n# Merging overall and by facility type\nyear1 &lt;- as.data.frame(dfyear)\nyear2 &lt;- as.data.frame(dfyearavg)\nmergedyearly &lt;- rbind(year1, year2)\nmergedyearly$Facility_Type &lt;- factor(mergedyearly$Facility_Type, levels = c(\"Integrated Network Cancer Program\",\n    \"Academic Program\", \"Overall Median\", \"Comprehensive CC Program\", \"Community Cancer Program\"))\n\n# plotting upfron surg by fac type\nggplot(mergedyearly) + geom_line(data = mergedyearly, aes(x = YEAR_OF_DIAGNOSIS,\n    y = ratio, group = Facility_Type, color = Facility_Type), size = 1.3) + labs(x = \"Year of Diagnosis\",\n    y = \"Proportion Receiving Upfront Surgery\", color = \"Facility Type\") + theme_test() +\n    scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\nCode\n# Graphing proportion receiving upfront surg at different facility types over time\nrcParams['figure.figsize'] = 8,6\n\ndfCC = synthetic_NCDB_data_py.query('FACILITY_TYPE_CD==1').filter(['YEAR_OF_DIAGNOSIS','neoadjuvant']).groupby(['YEAR_OF_DIAGNOSIS']).agg(['mean'])\ndfCCC = synthetic_NCDB_data_py.query('FACILITY_TYPE_CD==2').filter(['YEAR_OF_DIAGNOSIS','neoadjuvant']).groupby(['YEAR_OF_DIAGNOSIS']).agg(['mean'])\ndfA = synthetic_NCDB_data_py.query('FACILITY_TYPE_CD==3').filter(['YEAR_OF_DIAGNOSIS','neoadjuvant']).groupby(['YEAR_OF_DIAGNOSIS']).agg(['mean'])\ndfIN = synthetic_NCDB_data_py.query('FACILITY_TYPE_CD==4').filter(['YEAR_OF_DIAGNOSIS','neoadjuvant']).groupby(['YEAR_OF_DIAGNOSIS']).agg(['mean'])\n\nplt.figure(figsize = (8,6))\n\nplt.plot(dfCC, label=\"Community Cancer Center\")\nplt.plot(dfCCC, label=\"Comprehensive Community Cancer Center\")\nplt.plot(dfA, label=\"Academic Program\")\nplt.plot(dfIN, label=\"Integrated Cancer Program\")\nplt.legend()\nplt.xlabel(\"Year of Diagnosis\")\nplt.ylabel(\"Proportion Receiving Upfront Surgery\")\nplt.savefig('posts/first_project/overtime.png',bbox_inches='tight')\n\n\n\n\n\noutput\n\n\n\n\n\n\n\nDelays in Surgery and Facility Switching\nThis final figure displays how the counts of patients by how many days they have to wait till surgery, colored by whether they switched facilities or not.\n\nUsing RUsing Python\n\n\n\n\nCode\n# making three groups for a surgery delay\ndf2$surggroup[df2$DX_DEFSURG_STARTED_DAYS == 0] &lt;- \"Day zero\"\ndf2$surggroup[df2$DX_DEFSURG_STARTED_DAYS &gt; 0 & df2$DX_DEFSURG_STARTED_DAYS &lt; 28] &lt;- \"Short delay\"\ndf2$surggroup[df2$DX_DEFSURG_STARTED_DAYS &gt;= 28] &lt;- \"Long Delay\"\ndf2$surggroup &lt;- factor(df2$surggroup)\n\n# plotting surg delay and switching facility\nggplot(df2, aes(x = DX_DEFSURG_STARTED_DAYS, fill = treatnotatdiag)) + geom_histogram(binwidth = 5) +\n    xlim(-10, 200) + labs(x = \"Days Untill Surgery From Diagnosis\", y = \"Count\",\n    fill = \"Facility Switch\") + theme_test() + scale_fill_brewer(palette = \"Set2\") +\n    geom_vline(xintercept = 28)\n\n\n\n\n\n\n\n\n\nCode\n#Data frame that only includes people who got upfront surg\ndfsurg = synthetic_NCDB_data_py.query('neoadjuvant == 0')\n\n# Plotting surgery delay and switching facility\ndfsurggraph = dfsurg.filter(['treatnotatdiag','DX_DEFSURG_STARTED_DAYS']).query('DX_DEFSURG_STARTED_DAYS&lt;200')\ndfsurggraphswitch = dfsurggraph.query('treatnotatdiag == \"Switched\"')\ndfsurggraphstay = dfsurggraph.query('treatnotatdiag == \"Stayed\"')\nplt.clf()\nplt.hist(dfsurggraphswitch['DX_DEFSURG_STARTED_DAYS'], bins=40, fill=False,histtype='step',stacked=True,label=\"Switched\",alpha=0.5)\nplt.hist(dfsurggraphstay['DX_DEFSURG_STARTED_DAYS'], bins=40,fill=False,histtype='step',stacked=True,label=\"Stayed\",alpha=0.5)\nplt.xlabel(\"Days Untill Surgery From Diagnosis\")\nplt.ylabel(\"Count\")\nplt.legend()\nplt.axvline(x=28)\nplt.savefig('posts/first_project/switchvstay.png',bbox_inches='tight')\nplt.show()\n\n\n\n\n\noutput"
  },
  {
    "objectID": "posts/aacr_micro_cms_rna/index.html",
    "href": "posts/aacr_micro_cms_rna/index.html",
    "title": "AACR Poster: Microbiome and Tumor Gene Expression",
    "section": "",
    "text": "Check out my poster presentation at AACR 2023. Project code coming soon!"
  },
  {
    "objectID": "posts/collab_project_rna_seq_cms/index.html",
    "href": "posts/collab_project_rna_seq_cms/index.html",
    "title": "ASCO Poster: Genetic Ancestry and Gene Expression in CRC",
    "section": "",
    "text": "Check out my poster presentation at ASCO 2023. Project code coming soon!"
  },
  {
    "objectID": "posts/simple_conv/index.html",
    "href": "posts/simple_conv/index.html",
    "title": "Basic Convolutional Neural Network For Classification",
    "section": "",
    "text": "This is code I originally wrote for an assignment in my deep learning class. In order to post it on my public facing portfolio I’ve made several alterations and have changed the data set. The new data set I’m using is the “beans” data set from Hugging Face. This data set contains images of bean plant leaves with either no disease, Bean Rust, or Angular Leaf Spot. The leaf below has Bean Rust, identified by the small brown spots."
  },
  {
    "objectID": "posts/simple_conv/index.html#introduction",
    "href": "posts/simple_conv/index.html#introduction",
    "title": "Basic Convolutional Neural Network For Classification",
    "section": "",
    "text": "This is code I originally wrote for an assignment in my deep learning class. In order to post it on my public facing portfolio I’ve made several alterations and have changed the data set. The new data set I’m using is the “beans” data set from Hugging Face. This data set contains images of bean plant leaves with either no disease, Bean Rust, or Angular Leaf Spot. The leaf below has Bean Rust, identified by the small brown spots."
  },
  {
    "objectID": "posts/simple_conv/index.html#code",
    "href": "posts/simple_conv/index.html#code",
    "title": "Basic Convolutional Neural Network For Classification",
    "section": "Code",
    "text": "Code\n\nLoad Libraries\n\n\nCode\nfrom datasets import load_dataset, Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision.transforms import Compose, ColorJitter, ToTensor, Resize, RandomHorizontalFlip, RandomVerticalFlip, RandomRotation, RandomAffine\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport torch\nfrom time import time\nimport torch.utils.tensorboard as tb\nimport torch.nn.functional as F\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfrom torch import save\nfrom os import path\n\n\n\n\nMake Data set Class\nHere I turn the Hugging Hace data set into a PyTorch data set, which plays better with my previous CNN model and training set up. I perform a good amount of data augmentation including random flips and rotations, as well as jittering the brightness and contrast.\n\n\nCode\n# Inherits from pytorch datasets\nclass MyDataset(Dataset):\n    def __init__(self, data_path, train=True):\n        \n        # Only perform augmentation on the train data\n        if train:\n            self.dataset = load_dataset(\"imagefolder\", data_dir=data_path, drop_labels=False, split=\"train\")\n            self.dataset = self.dataset.map(self.img_resize, remove_columns=[\"image\"], batched=True)\n            # Here `set_transforms` performs the transforms on the fly to save memory (doesnt cache)\n            self.dataset.set_transform(self.transforms)\n            \n        # the valid/test data only gets turned into a pytorch tensor\n        else:\n            self.dataset = load_dataset(\"imagefolder\", data_dir=data_path, drop_labels=False, split=\"test\")\n            self.dataset = self.dataset.map(self.img_resize, remove_columns=[\"image\"], batched=True)\n            self.dataset.set_transform(self.test_transform)\n\n    def transforms(self, imgs):\n        augment = Compose([\n                            RandomHorizontalFlip(p=0.5), \n                            RandomVerticalFlip(p=0.5),\n                            ColorJitter(brightness=0.1,\n                                        contrast=0.1,\n                                        saturation=0.1,\n                                        hue=0),\n                                RandomRotation(degrees=45),\n                                RandomAffine(degrees=10),\n                                ToTensor()\n                            ])\n        imgs[\"pixel_values\"] = [augment(image) for image in imgs[\"pixel_values\"]]\n        return imgs\n\n    def test_transform(self, imgs):\n        augment = Compose([ToTensor()])\n        imgs[\"pixel_values\"] = [augment(image) for image in imgs[\"pixel_values\"]]\n        return imgs\n    \n    \n    def img_resize(self, imgs):\n        # Resize the images to save on memory usage\n        imgs[\"pixel_values\"] = [image.convert(\"RGB\").resize((100,100)) for image in imgs[\"image\"]]\n        return imgs\n\n    def __getitem__(self, index):\n        data = self.dataset[index]\n        # Make the labels one hot tensors w/data type float\n        label = F.one_hot(torch.tensor(data[\"label\"]), num_classes=3)\n        return data[\"pixel_values\"], label.float()\n\n    def __len__(self):\n        return len(self.dataset)\n\n#img = bean_data_train[0][\"pixel_values\"]\n#plt.imshow(np.transpose(img, (1,2,0)))\n\n\n\n\nBuild the CNN\nHere I build a pretty simply CNN with an initial layer with kernel size 7. The following layers can be added as “Blocks” for easy customization of channel numbers and network depth.\n\n\nCode\n\"\"\"\nCNN class\n - Can take layers argument to define number of channels and depth\n - Number of input channels will always be 3\n - Currently the first layer is hardcoded with kernel size 7, and stride 2\n    I think this should be reduced to 5 or even 3.\n - The class Block defines a block of 2 conv layers. This could be extended to 3\n    and include a skip. Could also include params for kernal size and striding\n - Normalization is performed here instead of in Utils, I randomly entered the mean and sdev so this may impact performance on the bean data set\n\"\"\"\n\nclass ConvoClassifier(torch.nn.Module):\n\n    class Block(torch.nn.Module):\n        def __init__(self, n_input, n_output, stride=1):\n            super().__init__()\n\n            # Defines a two layer block with stride in first layer only, batch norm after each\n            self.net = torch.nn.Sequential(\n                # Only the first layer is strided, can adjust this in the loop in the init method\n                torch.nn.Conv2d(n_input, n_output, kernel_size=3,\n                                padding=1, stride=stride),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(n_output, n_output, kernel_size=3, padding=1),\n                torch.nn.BatchNorm2d(n_output),\n                torch.nn.ReLU()\n            )\n\n        def forward(self, x):\n            return self.net(x)\n\n    def __init__(self, layers=[32, 64, 128], n_input_channels=3, n_classes=3):\n        super().__init__()\n        # Inital layer with kernal size 7, the max pool appears to increase accuracy on validation set\n        L = [torch.nn.Conv2d(n_input_channels, layers[0], kernel_size=7, padding=3, stride=2),\n             torch.nn.ReLU(), torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)]\n        c = layers[0]\n        # Build network from list of layers\n        for l in layers:\n            # can adjust stride here\n            L.append(self.Block(c, l, stride=2))\n            c = l\n\n        self.network = torch.nn.Sequential(*L)\n        # Linear layer at end for the 3 classification labels\n        self.classifier = torch.nn.Linear(c, n_classes)\n        # Mean and standard dev of color channels accross the entire training set\n        self.norm = torchvision.transforms.Normalize(\n            mean=[0.233, 0.298, 0.256], std=[0.199, 0.118, 0.201])\n\n    def forward(self, x):\n        # Normalize\n        normx = self.norm(x)\n        # Compute the features\n        z = self.network(normx)\n        # Global average pooling\n        z = z.mean(dim=[2, 3])\n        # Classify\n        return self.classifier(z)\n\n# Save the model with epoch number and message/name of model (for checkpoints)\ndef save_model(model, message, epoch):\n    name = message + '_' + str(epoch) + '_' + 'det.th'\n    from torch import save\n    from os import path\n    return save(model.state_dict(), path.join(path.dirname(path.abspath(__file__)), name))\n\ndef load_model(model_name):\n    from torch import load\n    from os import path\n    r = ConvoClassifier()\n    r.load_state_dict(load(path.join(path.dirname(\n        path.abspath(__file__)), model_name), map_location='cpu'))\n    return r\n\n\n\n\nTrain the Model\nThe main training loop with arguments that can be passed through the command line. I use tensor board to monitor training loss and validation accuracy.\n\n\nCode\n#from .models import ConvoClassifier, save_model, load_model\n#from .utils import MyDataset\n\n\"\"\"\nRunning tensorboard\n - launch terminal w/deeplearning virual env\n - run python -m tensorboard.main --logdir=runs\n - open in browser\n - enabels visualization of training loss and accuracy after each batch\n    and validation accuracy after each epoch\n\"\"\"\n\n\"\"\"\nMain training loop\n - Takes training arguments\n    - log_dir: directory of logs for tensorboard\n    - run_info: short description of run for identification in tensorboard\n    - lr: learning rate\n    - ep: number of epochs\n    - layers: takes multiple int values and constructs a list used for construction of model. \n        Each number is number of channels and length of list is number of layers\n\n - Prints time and validation accuracy to consol after each epoch. Saves model at end\n - Note: each \"layer\" is a block of 2 convolutional layers, see models.py\n - Should add ability to customize learning rate schedule, currently decaying around \n    6 epochs gives good results \n\"\"\"\n\ndef load_data(dataset_path, num_workers=0, batch_size=256, train=True):\n    dataset = MyDataset(dataset_path, train)\n    return DataLoader(dataset, num_workers=num_workers, batch_size=batch_size, shuffle=True, drop_last=True)\n\ndef train(args):\n\n    start = time()\n    # model constructed here\n    model = ConvoClassifier(args.layer_list, args.num_classes).to(device)\n\n    # set up logger with the run info name\n    train_logger, valid_logger = None, None\n    if args.log_dir is not None:\n        train_dir = \"train\" + args.run_info\n        valid_dir = \"valid\" + args.run_info\n        train_logger = tb.SummaryWriter(path.join(args.log_dir, train_dir))\n        valid_logger = tb.SummaryWriter(path.join(args.log_dir, valid_dir))\n\n    # Choice of optimizer, adam working better so far\n    # optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.learn_rate)\n\n    # LR scheduler, will want to eventually add ability to customize args for this\n    scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer, step_size=6, gamma=0.2)\n    step = 0\n\n    train_data = load_data(args.data_dir,0,args.batch_size,True)\n    val_data = load_data(args.data_dir,0,args.batch_size,False)\n\n    # Main loop\n    for epoch in range(args.num_epochs):\n        startepoch = time()\n        total_loss = 0\n\n        # Make sure things are set to training mode\n        model.train()\n        for i, (x, y) in enumerate(train_data):\n\n            x = x.to(device)\n            y = y.to(device)\n            output = model(x)\n            l = F.cross_entropy(output, y)\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            total_loss += l\n\n            # compute accuracy on training batch, need to get it back to CPU and change to numpy array\n            acc = (output.argmax(1).type_as(y) ==\n                   y.argmax(1)).float().detach().cpu().numpy()\n            acc = np.mean(acc)\n\n            train_logger.add_scalar(\"Loss\", l, global_step=step)\n            #train_logger.add_scalar(\"acc\", acc, global_step=step)\n            step += 1\n\n        # Test model on validation set after training epoch, make sure to set to eval mode\n        model.eval()\n        val_acc = np.array([])\n        for i, (x, y) in enumerate(val_data):\n            x = x.to(device)\n            y = y.to(device)\n            output = model(x)\n            # compute accuracy on validation set, need to get it back to CPU and change to numpy array\n            acc = (output.argmax(1).type_as(y) ==\n                   y.argmax(1)).float().detach().cpu().numpy()\n            acc = np.mean(acc)\n            val_acc = np.append(val_acc, acc)\n           \n        valid_logger.add_scalar(\n            \"val_acc_epoch\", np.mean(val_acc), global_step=step)\n       \n        # End of epoch, print validation accurcy and epoch time\n        endepoch = time()\n        scheduler.step()\n        print(np.mean(val_acc))\n        print(\"epochtime\", endepoch-startepoch)\n\n    # print total time of model and save\n    end = time()\n    print(\"total time\", end-start)\n    save_model(model, args.run_info, args.num_epochs)\n\n\"\"\"\nArguments:\n - log_dir: directory of logs for tensorboard\n - run_info: short description of run for identification in tensorboard\n - lr: learning rate\n - ep: number of epochs\n - layers: takes multiple int values and constructs a list used for construction of model. \n    Each number is number of channels and length of list is number of layers\n\"\"\"\n\nif __name__ == '__main__':\n    import argparse\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--log_dir', default='runs')\n    parser.add_argument('-n', '--run_info', type=str)\n    parser.add_argument('-lr', '--learn_rate', type=float, default=0.0001)\n    parser.add_argument('-ep', '--num_epochs', type=int, default=4)\n    # layer list requires at least one number. Multiple numbers seperated by a single space\n    parser.add_argument('-layers', '--layer_list', nargs='+',\n                        type=int, default=[32, 64, 128])\n    parser.add_argument('-data', '--data_dir', type=str, default='../../beans')\n    parser.add_argument('-c', '--num_classes', type=int, default=3 )\n    parser.add_argument('-bs', '--batch_size', type=int, default=256)\n\n    args = parser.parse_args()\n    train(args)"
  },
  {
    "objectID": "posts/simple_conv/index.html#results",
    "href": "posts/simple_conv/index.html#results",
    "title": "Basic Convolutional Neural Network For Classification",
    "section": "Results",
    "text": "Results\n\nCommand line calls\nI try training two models with different numbers of channels in each layer. Which will perform better?\npython -m run_beans -n small_model -bs 128 -ep 12 -layers 32 64 128\npython -m run_beans -n big_model -bs 128 -ep 12 -layers 64 128 256\n\n\nTraining Loss\nMy initial results didn’t seem too bad! This was done with a very minimal amount of “Graduate student descent” so the model has a ton of room for improvement.\nThe smaller model is in orange and the larger model is in purple. The larger model has lower training loss after 12 epochs but does it have better accuracy?\n\n\n\n\n\n\n\n\n\n\n\n\nValidation Accuracy\nHere the larger model is in green and the smaller model is in grey. Looks like the larger model is a bit better! However after about 9 epochs the accuracy starts to plateu. I’d want to do some graduate student descent to get that accuracy up (But not push it too hard as I don’t want to over fit to the validation set). Also, if this were a more serious project I’d need to have a hold out test set that I only try at the very end of training. (No peeking!)"
  }
]